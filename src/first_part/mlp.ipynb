{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "import os\n",
    "from typing import List, Tuple, Dict, Union\n",
    "from matrices import confusion_matrix, plot_confusion_matrix\n",
    "from metrics import calculate_metrics, update_results, build_summary, plot_leaning_curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv_data(filepath: str, columns: List[str], sep: str = ',') -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Carrega o dataset EMG e organiza as colunas.\n",
    "\n",
    "    Args:\n",
    "        filepath (str): Caminho para o arquivo.\n",
    "        columns (List[str]): Lista com os nomes das colunas.\n",
    "        transpose (bool): Transpor o DataFrame.\n",
    "        sep (str): Separador dos dados.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame com dados dos sensores e classes.\n",
    "    \"\"\"\n",
    "    \n",
    "    df: pd.DataFrame = pd.read_csv(filepath, header=None, sep=sep)\n",
    "    \n",
    "    df.columns = columns\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          x         y  spiral\n",
      "0  15.40724  -3.66801     1.0\n",
      "1  15.07298  -1.56346     1.0\n",
      "2 -15.43986   0.16502    -1.0\n",
      "3  -9.26071  12.24981    -1.0\n",
      "4   7.59201   7.56913    -1.0\n"
     ]
    }
   ],
   "source": [
    "columns = [\"x\", \"y\", \"spiral\"]\n",
    "\n",
    "df: pd.DataFrame = load_csv_data(filepath = \"../../resources/spiral.csv\", columns = columns)\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(df: pd.DataFrame) -> None:\n",
    "    plt.figure(figsize=(8, 6))\n",
    "\n",
    "    plt.scatter(df[df['spiral'] == 1.0]['x'], \n",
    "                df[df['spiral'] == 1.0]['y'], \n",
    "                color='blue', label='Classe 1.0', alpha=0.7)\n",
    "\n",
    "    plt.scatter(df[df['spiral'] == -1.0]['x'], \n",
    "                df[df['spiral'] == -1.0]['y'], \n",
    "                color='red', label='Classe -1.0', alpha=0.7)\n",
    "\n",
    "    plt.title('Gráfico de Dispersão (Espalhamento)', fontsize=16)\n",
    "    plt.xlabel('X', fontsize=14)\n",
    "    plt.ylabel('Y', fontsize=14)\n",
    "\n",
    "    plt.legend()\n",
    "\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_data(data_frame: pd.DataFrame) -> None:\n",
    "    \"\"\"Plota os dados de treinamento.\"\"\"\n",
    "    plt.scatter(data_frame[data_frame['spiral'] == 1.0]['x'],\n",
    "                data_frame[data_frame['spiral'] == 1.0]['y'], \n",
    "                color='blue', label='Classe 1.0', alpha=0.7)\n",
    "    plt.scatter(data_frame[data_frame['spiral'] == -1.0]['x'], \n",
    "                data_frame[data_frame['spiral'] == -1.0]['y'], \n",
    "                color='red', label='Classe -1.0', alpha=0.7)\n",
    "    plt.title('Treinamento do Adaline - Linha de Decisão', fontsize=16)\n",
    "    plt.xlabel('X', fontsize=14)\n",
    "    plt.ylabel('Y', fontsize=14)\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.xlim(-20, 20)\n",
    "    plt.ylim(-20, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_decision_boundary(W: np.ndarray, x_axis: np.ndarray) -> None:\n",
    "    \"\"\"Atualiza a linha de decisão no gráfico.\"\"\"\n",
    "    if W[2, 0] != 0:\n",
    "        x2 = -W[1, 0] / W[2, 0] * x_axis + W[0, 0] / W[2, 0]\n",
    "        x2 = np.nan_to_num(x2)\n",
    "        plt.plot(x_axis, x2, color='orange', alpha=0.1)\n",
    "        plt.pause(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_final_decision_boundary(W: np.ndarray, x_axis: np.ndarray) -> None:\n",
    "    \"\"\"Plota a linha de decisão final.\"\"\"\n",
    "    if W[2, 0] != 0:\n",
    "        x2 = -W[1, 0] / W[2, 0] * x_axis + W[0, 0] / W[2, 0]\n",
    "        x2 = np.nan_to_num(x2)\n",
    "        plt.plot(x_axis, x2, color='green', linewidth=2)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(\n",
    "    df: pd.DataFrame, \n",
    "    input_columns: List[str], \n",
    "    target_column: str,\n",
    "    transpose: bool = False,\n",
    "    normalize: bool = False,\n",
    "    test_size: float = 0.2, \n",
    "    random_state: int = 42,\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Prepara o conjunto de dados para redes neurais, organizando entradas, saídas e divisões.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame contendo o conjunto de dados.\n",
    "        input_columns (list[str]): Lista com os nomes das colunas de entrada.\n",
    "        target_column (str): Nome da coluna de saída (rótulos).\n",
    "        transpose (bool): Transpor o DataFrame. Default é False.\n",
    "        test_size (float): Proporção do conjunto de teste (0 a 1). Default é 0.2 (20%).\n",
    "        random_state (int): Semente para reprodutibilidade da divisão. Default é 42.\n",
    "\n",
    "    Returns:\n",
    "        dict: Um dicionário contendo os conjuntos organizados:\n",
    "            - 'X_train': Entradas para treinamento.\n",
    "            - 'X_test': Entradas para teste.\n",
    "            - 'Y_train': Rótulos para treinamento.\n",
    "            - 'Y_test': Rótulos para teste.\n",
    "    \"\"\"\n",
    "    X = df[input_columns].values\n",
    "    Y = df[target_column].values\n",
    "    \n",
    "    if transpose:\n",
    "        X = X.T\n",
    "        Y = Y.T\n",
    "        \n",
    "        p, N = X.shape\n",
    "        X = np.concatenate((\n",
    "            -np.ones((1, N)),\n",
    "            X\n",
    "        ))\n",
    "        \n",
    "    else:\n",
    "        N, p = X.shape\n",
    "        X = np.concatenate((\n",
    "            -np.ones((N, 1)),\n",
    "            X\n",
    "        ), axis = 1)\n",
    "    \n",
    "    if normalize:\n",
    "        X = 2 * (X - X.min()) / (X.max() - X.min()) - 1\n",
    "\n",
    "    return {\n",
    "        'X': X,\n",
    "        'Y': Y,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1999, 3)\n",
      "(1999,)\n"
     ]
    }
   ],
   "source": [
    "data = prepare_data(\n",
    "    df=df, \n",
    "    input_columns=[\"x\", \"y\"], \n",
    "    target_column=\"spiral\",\n",
    "    transpose=True,\n",
    "    normalize=True,\n",
    "    test_size=0.2, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(data['X'].shape)\n",
    "print(data['Y'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(predictions: np.ndarray, Y_test: np.ndarray) -> (float, float, float):\n",
    "    \"\"\"\n",
    "    Calcula metricas de desempenho para o modelo.\n",
    "    \n",
    "    Args:\n",
    "        predictions (np.ndarray): Vetor de predições.\n",
    "        Y_test (np.ndarray): Vetor de rótulos reais.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple[float, float, float]: Acurácia, Sensibilidade e Especificidade.\n",
    "    \"\"\"\n",
    "    \n",
    "    acuracy = np.mean(predictions == Y_test)\n",
    "    true_positive = np.sum((predictions == 1) & (Y_test == 1))\n",
    "    true_negative = np.sum((predictions == -1) & (Y_test == -1))\n",
    "    false_positive = np.sum((predictions == 1) & (Y_test == -1))\n",
    "    false_negative = np.sum((predictions == -1) & (Y_test == 1))\n",
    "    \n",
    "    sensitivity = true_positive / (true_positive + false_negative) if (true_positive + false_negative) > 0 else 0\n",
    "    specificity = true_negative / (true_negative + false_positive) if (true_negative + false_positive) > 0 else 0\n",
    "    \n",
    "    return acuracy, sensitivity, specificity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_results(\n",
    "    metrics: Tuple[float, float, float],\n",
    "    results: List[Dict[str, Union[float, np.ndarray]]],\n",
    "    predictions: np.ndarray,\n",
    "    Y_test: np.ndarray,\n",
    "    mse_history: List[float]\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Atualiza os resultados do modelo.\n",
    "        \n",
    "    Args:\n",
    "        metrics (Tuple[float, float, float]): Acurácia, Sensibilidade e Especificidade.\n",
    "        results (List[Dict[str, Union[float, np.ndarray]]]): Lista de resultados.\n",
    "        predictions (np.ndarray): Vetor de predições.\n",
    "        Y_test (np.ndarray): Vetor de rótulos reais.\n",
    "        mse_history (List[float]): Lista de erros médios quadráticos.\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    \n",
    "    accuracy, sensitivity, specificity = calculate_metrics(predictions, y_test)\n",
    "\n",
    "    metrics[\"accuracy\"].append(accuracy)\n",
    "    metrics[\"sensitivity\"].append(sensitivity)\n",
    "    metrics[\"specificity\"].append(specificity)\n",
    "\n",
    "    results.append({\"accuracy\": accuracy, \"conf_matrix\": conf_matrix(y_test, predictions), \"mse\": mse_history})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_root_mean_square_error(\n",
    "    X_train: np.ndarray,\n",
    "    Y_train: np.ndarray,\n",
    "    w: np.ndarray\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Calcula o erro quadrático médio.\n",
    "\n",
    "    Args:\n",
    "        X_train (np.ndarray): Entradas para treinamento.\n",
    "        Y_train (np.ndarray): Rótulos para treinamento.\n",
    "        w (np.ndarray): Vetor de pesos.\n",
    "\n",
    "    Returns:\n",
    "        float: Erro quadrático médio.\n",
    "    \"\"\"\n",
    "    p_1, N = X_train.shape\n",
    "    square_error = 0\n",
    "\n",
    "    for t in range(N):\n",
    "        x_t = X_train[:, t].reshape(p_1, 1)\n",
    "        u_t = (w.T @ x_t)[0, 0]\n",
    "        d_t = Y_train[0, t]\n",
    "        square_error += (d_t - u_t)**2\n",
    "\n",
    "    return square_error / (2 * N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation_function(\n",
    "    u: np.ndarray, \n",
    "    logistic: bool = True, \n",
    "    hyperbolic: bool = False\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Função de ativação para a rede MLP. Pode ser logística ou hiperbólica.\n",
    "    \n",
    "    Args:\n",
    "        u (np.ndarray): Vetor de entradas.\n",
    "        logistic (bool): Função logística. Default é True.\n",
    "        hyperbolic (bool): Função hiperbólica. Default é False.\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: Vetor de saídas.\n",
    "    \"\"\"\n",
    "    \n",
    "    if logistic:\n",
    "        return (u - np.min(u)) / (np.max(u) - np.min(u))\n",
    "    \n",
    "    if hyperbolic:\n",
    "        return 2 * ((u - np.min(u)) / (np.max(u) - np.min(u))) - 1\n",
    "    \n",
    "    raise ValueError(\"Either 'logistic' or 'hyperbolic' must be True.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation_derivate(\n",
    "    u: np.ndarray, \n",
    "    logistic: bool = True, \n",
    "    hyperbolic: bool = False\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Derivada da função de ativação para a rede MLP. Pode ser logística ou hiperbólica.\n",
    "    \n",
    "    Args:\n",
    "        u (np.ndarray): Vetor de entradas.\n",
    "        logistic (bool): Função logística. Default é True.\n",
    "        hyperbolic (bool): Função hiperbólica. Default é False.\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: Vetor de saídas.\n",
    "    \"\"\"    \n",
    "    if logistic:\n",
    "        return u * (1 - u)\n",
    "    \n",
    "    if hyperbolic:\n",
    "        return 1 - u**2\n",
    "    \n",
    "    raise ValueError(\"Either 'logistic' or 'hyperbolic' must be True.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_train(\n",
    "    data: np.ndarray,\n",
    "    labels: np.ndarray,\n",
    "    hidden_units: int,\n",
    "    last_layer_units: int = 1,\n",
    "    learning_rate: float = 0.01,\n",
    "    epochs: int = 100,\n",
    "    tolerance: float = 1e-3,\n",
    "    patience: int = 10,\n",
    "    transpose: bool = False\n",
    "):\n",
    "    \"\"\"\n",
    "    Treina uma rede MLP.\n",
    "    \n",
    "    Args:\n",
    "        data (np.ndarray): Dados de entrada.\n",
    "        labels (np.ndarray): Rótulos.\n",
    "        hidden_units (int): Número de neurônios na camada oculta.\n",
    "        last_layer_units (int): Número de neurônios na última camada. Default é 1.\n",
    "        learning_rate (float): Taxa de aprendizado. Default é 0.01.\n",
    "        epochs (int): Número máximo de épocas. Default é 100.\n",
    "        tolerance (float): Tolerância para convergência. Default é 1e-3.\n",
    "        patience (int): Paciência para early stopping. Default é 10.\n",
    "        transpose (bool): Transpor o conjunto de dados. Default é False.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: Tupla contendo os pesos treinados e o histórico de erro.\n",
    "    \"\"\"\n",
    "    \n",
    "    if transpose:\n",
    "        data = data.T\n",
    "        labels = labels.T\n",
    "    \n",
    "    N, p = data.shape\n",
    "    \n",
    "    layers = [N] + hidden_units + [last_layer_units]\n",
    "    \n",
    "    weights = [np.random.randn(layers[i + 1], layers[i] + 1) * np.sqrt(2 / layers[i]) for i in range(len(layers) - 1)]\n",
    "    \n",
    "    mse_history = []\n",
    "    no_improvement = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Forward pass\n",
    "        activations = [data]\n",
    "        z_s = []\n",
    "        \n",
    "        for w in range(len(weights)):\n",
    "            a_with_bias = np.vstack([np.ones((1, activations[-1].shape[1])), activations[-1]])\n",
    "            z = np.dot(w, a_with_bias)\n",
    "            z_s.append(z)\n",
    "            a = activation_function(u=z, logistic=False, hyperbolic=True)\n",
    "            activations.append(a)\n",
    "\n",
    "        y_pred = activations[-1]\n",
    "        mse = np.mean(np.sum((y_pred - labels) ** 2, axis=1))\n",
    "        mse_history.append(mse)\n",
    "        \n",
    "        if epoch > 0 and mse >= mse_history[-2]:\n",
    "            no_improvement += 1\n",
    "            if no_improvement >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch}.\")\n",
    "                break\n",
    "        else:\n",
    "            no_improvement = 0\n",
    "        \n",
    "        if epoch > 0 and abs(mse_history[-1] - mse_history[-2]) < tolerance:\n",
    "            print(f\"Converged at epoch {epoch}.\")\n",
    "            break\n",
    "        \n",
    "        # Backpropagation\n",
    "        deltas = [None] * len(weights)\n",
    "        deltas[-1] = (y_pred - labels) * activation_derivate(u=y_pred, logistic=False, hyperbolic=True)\n",
    "        \n",
    "        for l in range(len(weights) - 2, -1, -1):\n",
    "            a_with_bias = np.vstack([np.ones((1, activations[l + 1].shape[1])), activations[l + 1]])\n",
    "            deltas[l] = np.dot(weights[l + 1][:, 1:].T, deltas[l + 1]) * activation_derivate(u=activations[l + 1], logistic=False, hyperbolic=True)\n",
    "        \n",
    "        for l in range(len(weights)):\n",
    "            a_with_bias = np.vstack([np.ones((1, activations[l].shape[1])), activations[l]])\n",
    "            weights[l] -= learning_rate * np.dot(deltas[l], a_with_bias.T) / p\n",
    "    \n",
    "    return weights, mse_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_predict(\n",
    "    data: np.ndarray,\n",
    "    weights: List[np.ndarray]\n",
    "):\n",
    "    activations = data\n",
    "    \n",
    "    for w in range(len(weights)):\n",
    "        a_with_bias = np.vstack([np.ones((1, activations.shape[1])), activations])\n",
    "        z = np.dot(w, a_with_bias)\n",
    "        activations = activation_function(u=z, logistic=False, hyperbolic=True)\n",
    "    \n",
    "    final_output = activations\n",
    "    predictions = np.argmax(final_output, axis=1)\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m hidden_units \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m8\u001b[39m]\n\u001b[1;32m      7\u001b[0m last_layer_units \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m----> 8\u001b[0m data, labels \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m, data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mY\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      9\u001b[0m separe_data_with_sklearn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Monte Carlo\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": [
    "R = 50\n",
    "epochs = 1000\n",
    "learning_rate = 0.01\n",
    "tolerance = 1e-3\n",
    "patience = 10\n",
    "hidden_units = [8, 8, 8, 8, 8, 8, 8, 8]\n",
    "last_layer_units = 1\n",
    "data, labels = data['X'], data['Y']\n",
    "separe_data_with_sklearn = False\n",
    "\n",
    "# Monte Carlo\n",
    "n_samples = data.shape[1]\n",
    "metrics = {\"accuracy\": [], \"sensitivity\": [], \"specificity\": []}\n",
    "results = []\n",
    "\n",
    "for i in range(R):\n",
    "    indices = np.arange(n_samples)\n",
    "    np.random.shuffle(indices)\n",
    "    data, labels = data[indices], labels[indices]\n",
    "    \n",
    "    if separe_data_with_sklearn:\n",
    "        X_train, X_test, Y_train, Y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "    else:\n",
    "        N_train = int(0.8 * n_samples)\n",
    "        X_train, Y_train = data[:N_train], labels[:N_train]\n",
    "        X_test, Y_test = data[N_train:], labels[N_train:]\n",
    "    \n",
    "    weights, mse_history = mlp_train(\n",
    "        data=X_train,\n",
    "        labels=Y_train,\n",
    "        hidden_units=hidden_units,\n",
    "        last_layer_units=last_layer_units,\n",
    "        learning_rate=learning_rate,\n",
    "        epochs=epochs,\n",
    "        tolerance=tolerance,\n",
    "        patience=patience,\n",
    "        transpose=False\n",
    "    )\n",
    "    predictions = mlp_predict(data=X_test, weights=weights)\n",
    "    update_results(metrics=metrics, results=results, predictions=predictions, Y_test=Y_test, mse_history=mse_history)\n",
    "\n",
    "    if (i + 1) % 10 == 0:\n",
    "        print(f\"Finished iteration {i + 1}.\")\n",
    "\n",
    "bestMlp = results[np.argmax([r[\"accuracy\"] for r in results])]\n",
    "worstMlp = results[np.argmin([r[\"accuracy\"] for r in results])]\n",
    "summary = build_summary(metrics)\n",
    "\n",
    "print(f\"\\n Resume of MLP metrics: \\n\")\n",
    "print(summary)\n",
    "plot_confusion_matrix(\n",
    "    bestMlp[\"conf_matrix\"],\n",
    "    worstMlp[\"conf_matrix\"],\n",
    "    \"MLP\"\n",
    ")\n",
    "plot_leaning_curves(\n",
    "    bestMlp[\"mse\"],\n",
    "    worstMlp[\"mse\"],\n",
    "    \"MLP\"\n",
    ")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FacialRecognition",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
